{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1po66F-YT9VFrxGl5fwub6igBNQhLS6pg",
      "authorship_tag": "ABX9TyPKwzNLTKYVpI1joaLH9TBW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nirisha444/Worksheet/blob/main/Worsheet_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "– Objective of the Task -\n",
        "To Predict the marks obtained in writing based on the marks of Math and Reading.\n",
        "• To - Do - 1:\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ],
      "metadata": {
        "id": "LjHe6J04IVcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Worksheet/student.csv\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nTop 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows:\")\n",
        "print(df.tail())\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "X = df[['Math', 'Reading']]\n",
        "Y = df['Writing']\n",
        "\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel (Y):\")\n",
        "print(Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQa8SlDlIYSP",
        "outputId": "ebc96363-1868-4c52-d569-0123c3d6abd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]\n",
            "\n",
            "Top 5 rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 2:\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n",
        "3. Note: The feature matrix described above does not include a column of 1s, as it assumes the\n",
        "absence of a bias term in the model."
      ],
      "metadata": {
        "id": "ow5niXrcQICL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Worksheet/student.csv')\n",
        "\n",
        "X = data[['Math', 'Reading']].values.T\n",
        "\n",
        "Y = data['Writing'].values\n",
        "\n",
        "d = X.shape[0]\n",
        "W = np.zeros(d)\n",
        "\n",
        "Y_pred = np.dot(W.T, X)\n",
        "\n",
        "print(\"Shape of X (features x samples):\", X.shape)\n",
        "print(\"Shape of W (weights):\", W.shape)\n",
        "print(\"Shape of Y_pred (predictions):\", Y_pred.shape)\n",
        "print(\"First 5 predicted values:\", Y_pred[:5])"
      ],
      "metadata": {
        "id": "NpeKYJ8pQhU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cd7c27-be0e-49cf-dad4-4adc66697da4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (features x samples): (2, 1000)\n",
            "Shape of W (weights): (2,)\n",
            "Shape of Y_pred (predictions): (1000,)\n",
            "First 5 predicted values: [0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "K7U04iP0KzqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['Math', 'Reading']]\n",
        "Y = df['Writing']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set (X):\")\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjKVizCnK04z",
        "outputId": "bc94f32d-85d9-4179-d0d8-f8d9a081897b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set (X):\n",
            "     Math  Reading\n",
            "29     64       82\n",
            "535    62       70\n",
            "695    36       21\n",
            "557    81       70\n",
            "836    82       86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 4:\n",
        "Building a Cost Function:"
      ],
      "metadata": {
        "id": "Uay3gHdNlnh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "   m = len(Y)\n",
        "   J = np.sum((X.dot(W)-Y) ** 2/(2 * m))\n",
        "   return J"
      ],
      "metadata": {
        "id": "XxIQvmGZl06M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 5:Testing a Cost Function:"
      ],
      "metadata": {
        "id": "G_z9rFG5m9fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "if cost == 0:\n",
        "   print(\"Proceed Further\")\n",
        "else:\n",
        "   print(\"something went wrong: Reimplement a cost function\")\n",
        "print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU4WqKOlm9-W",
        "outputId": "9d52804c-67ff-4476-8641-a433314a3df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 6:\n",
        "Implement your code for Gradient Descent; Either fill the following code or write your own:\n",
        "\n",
        "Gradient Descent from Scratch:"
      ],
      "metadata": {
        "id": "Bihk2A9Gsk56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "Parameters:\n",
        "X (numpy.ndarray): Feature matrix (m x n).\n",
        "Y (numpy.ndarray): Target vector (m x 1).\n",
        "W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "alpha (float): Learning rate.\n",
        "iterations (int): Number of iterations for gradient descent.\n",
        "Returns:\n",
        "tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        ".\n",
        "W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "cost_history (list): History of cost values over iterations.\n",
        "\"\"\"\n",
        "  # Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        "  # Number of samples\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "    # Step 1: Hypothesis Values\n",
        "    Y_pred = X.dot(W)\n",
        "    # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "    # Step 3: Gradient Calculation\n",
        "    dw = (X.T.dot(loss)) / (m)\n",
        "    # Step 4: Updating Values of W using Gradient\n",
        "    W_update = W - alpha * dw\n",
        "    # Step 5: New Cost Value\n",
        "    cost = cost_function(X, Y, W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history"
      ],
      "metadata": {
        "id": "QlMfMQDQGorG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 7:\n",
        "Make sure following Test Case is passe by your code from To - Do - 6 or your Gradient Descent\n",
        "Implementation:\n",
        "\n",
        "Test Code for Gradient Descent function:"
      ],
      "metadata": {
        "id": "WIVR7Jsusxhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd8lcXlxs3oU",
        "outputId": "cd807d3d-0e1e-45e0-8a70-954cef0afd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.3996496  0.92745322 0.09826523]\n",
            "Cost History: [np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155), np.float64(0.10711197094660155)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 8:\n",
        "Code for RMSE:"
      ],
      "metadata": {
        "id": "-5TKq60atCaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "\"\"\"\n",
        "This Function calculates the Root Mean Squres.\n",
        "Input Arguments:\n",
        "Y: Array of actual(Target) Dependent Varaibles.\n",
        "Y_pred: Array of predeicted Dependent Varaibles.\n",
        "Output Arguments:\n",
        "rmse: Root Mean Square.\n",
        "\"\"\"\n",
        "def rmse(Y, Y_pred):\n",
        "  rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "  return rmse"
      ],
      "metadata": {
        "id": "cd8PAbAWLIGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 9 - Implementation in the Code:\n",
        "Code for R-Squared Error:"
      ],
      "metadata": {
        "id": "bay993KcusOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "\"\"\"\n",
        "This Function calculates the R Squared Error.\n",
        "Input Arguments:\n",
        "Y: Array of actual(Target) Dependent Varaibles.\n",
        "Y_pred: Array of predeicted Dependent Varaibles.\n",
        "Output Arguments:\n",
        "rsquared: R Squared Error.\n",
        "\"\"\"\n",
        "def r2(Y, Y_pred):\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "  ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res / ss_tot)\n",
        "  return r2"
      ],
      "metadata": {
        "id": "QFU0OkQVTTMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 10:\n",
        "We will define a function that:\n",
        "1. Loads the data and splits it into training and test sets.\n",
        "2. Prepares the feature matrix (X) and target vector (Y).\n",
        "3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
        "4. Calls the gradient descent function to learn the parameters.\n",
        "5. Evaluates the model using RMSE and R2"
      ],
      "metadata": {
        "id": "RMYo9_N9u0SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Worksheet/student.csv')\n",
        "\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1]) # Initialize weights\n",
        "    alpha = 0.00001 # Learning rate\n",
        "    iterations = 1000 # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "    #Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "#Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqemEyrEu-e1",
        "outputId": "6372d21f-f2c1-4f5a-ee99-cd12dff1959f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.04797833 0.05020199]\n",
            "Cost History (First 10 iterations): [np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755), np.float64(2013.165570783755)]\n",
            "RMSE on Test Set: 63.36563528655014\n",
            "R-Squared on Test Set: -15.04041794561271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 11 - Present your finding:\n",
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n"
      ],
      "metadata": {
        "id": "IKDzJK7TzHZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When alpha = 0.00001, The model is underfitting as the RMSE value is very high and R-square is negative. The cost history is slao constant which states Gradient descent did not converge properly."
      ],
      "metadata": {
        "id": "n2-8GT644I17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result."
      ],
      "metadata": {
        "id": "sZQHQW744RXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpha = 0.0001: Final Weights: [0.47978325 0.50201988] Cost History (First 10 iterations): [np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211)] RMSE on Test Set: 6.092665443799174 R-Squared on Test Set: 0.851706281452247\n",
        "\n",
        "When alpha value is increased from 0.00001 to 0.0001, the model perfomance improved. The RMSE reduced from 63.36 to 6.09, and the R² score increased from −15.04 to 0.85, indicating that the model learned effectively. So, the model perfomance is acceptable neither underfitting nor overfitting.\n",
        "\n",
        "aplha = 0.001: Final Weights: [4.7978325 5.02019875] Cost History (First 10 iterations): [np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246)] RMSE on Test Set: 615.8660062668006 R-Squared on Test Set: -1514.236975362205\n",
        "\n",
        "The model is unstable and performs poorly."
      ],
      "metadata": {
        "id": "rbdX3TAN4V8w"
      }
    }
  ]
}